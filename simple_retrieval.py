import argparse
import asyncio
import json
import os
import re
from typing import Dict, List, Optional, Set, Tuple

import faiss
import numpy as np
from openai import OpenAI, RateLimitError
from sentence_transformers import SentenceTransformer

from designer import OPENAI_API_KEY


class SimpleRetrieval:
    def __init__(self, index_file: str = 'new_inverse_index.json', embeddings_file: str = 'embeddings.npy', item_id_file: str = 'item_ids.npy'):
        self.index_file = index_file
        self.embeddings_file = embeddings_file
        self.index: Dict[str, Set[str]] = {}
        self.items: Dict[str, Dict] = {}
        self.faiss_index = None
        self.item_id_file = item_id_file
        self.item_ids = []
        self.embeddings = None
        self.client = OpenAI(api_key=OPENAI_API_KEY)
        self.model = SentenceTransformer('all-MiniLM-L6-v2')

        with open("image_embedding_data.json", "r") as f:
            data = json.load(f)
            self.data_map = {item["item_id"]: item for item in data}

        with open("mapping_3d_spins.json", "r") as f:
            self.image_mapping = json.load(f)

    async def process_query(self, query_object: Dict[str, str]) -> Tuple[str, str]:
        """Process the query object and generate boolean query and description"""
        try:
            prompt = f"""
            You are an AI assistant that generates **Boolean search queries** and **object descriptions** for furniture items based on user requests. Given a user's natural language input and structured product data, your task is to:

            ### 1. **Generate a Boolean Search Query**  
            Create a search query using logical operators (**AND**, **OR**, **NOT**) to filter items based on single-word indexes.

            - Use **AND** to connect keywords from user conversation.
            - Do NOT use other attributes like material or style to build the boolean query.
            - Use **OR** for synonyms or alternative preferences.
            - Use **NOT** to exclude attributes the user explicitly does not want.
            - Only include **single-word tokens** (no phrases or descriptive language).
            - Keep the query flexible and relevant to the user's intent.

            ### 2. **Generate an Object Description**  
            Create a detailed object description that will be used for **ranking** and **presentation**. This should:

            - Reflect **user preferences**, including implicit ones.
            - Enrich the description with available structured data:  
              - `"material"`  
              - `"color"`  
              - `"style"`  
              - `"item_keywords"`  
              - `"description"`  
            - Include specific shape, material, room context, design style, exclusions, and other ranking-relevant keywordsâ€”even if not explicitly mentioned by the user.

            ### User Input:
            User Conversation: {query_object['user_query']}
            Material: {query_object['material']}
            Style: {query_object['style']}
            Keywords: {query_object['keywords']}

            Please provide:
            1. A Boolean search query using AND, OR, NOT operators
            2. A detailed object description for ranking
            """

            # Call OpenAI API to generate the query and description
            response = await asyncio.to_thread(
                self.client.chat.completions.create,
                model="gpt-4o",
                messages=[
                    {"role": "system",
                     "content": "You are an AI assistant that generates Boolean search queries and object "
                                "descriptions for furniture items."},
                    {"role": "user", "content": prompt}
                ],
                seed=42,
                max_tokens=500,
                temperature=0,
            )

            # Parse the response to get boolean query and description
            response_text = response.choices[0].message.content

            # Extract boolean query and description using more robust parsing
            parts = response_text.split("\n")
            boolean_query = ""
            object_description = ""
            
            for part in parts:
                if "Boolean Query:" in part:
                    boolean_query = part.split("Boolean Query:")[1].strip()
                elif "Object Description:" in part:
                    object_description = part.split("Object Description:")[1].strip()
                elif boolean_query == "" and "AND" in part or "OR" in part or "NOT" in part:
                    boolean_query = part.strip()
                elif object_description == "" and len(part) > 50:  # Assuming descriptions are longer
                    object_description = part.strip()

            # If parsing failed, use the original method as fallback
            if not boolean_query or not object_description:
                boolean_query = response_text.split("Boolean Query:")[1].split("Object Description:")[0].strip()
                object_description = response_text.split("Object Description:")[1].strip()
            print("boolean_query", boolean_query)
            print("object_description", object_description)
            return boolean_query, object_description

        except RateLimitError as e:
            print(f"Rate limit reached: {e}")
            raise
        except Exception as e:
            print(f"Error processing query: {e}")
            raise

    async def get_embedding(self, text: str) -> np.ndarray:
        """Get embedding for the given text"""
        try:
            embedding = self.model.encode(text)
            return np.array(embedding)
        except RateLimitError as e:
            print(f"Rate limit reached: {e}")
            raise
        except Exception as e:
            print(f"Error getting embedding: {e}")
            raise

    async def retrieve_with_query_object(self, user_input: Dict[str, str], k: int = 10) -> List[Dict[str, str]]:
        """Process query object and retrieve results"""
        try:
            # Generate boolean query and description
            itemId = user_input["selectedItemId"]
            material = self.data_map[itemId]["material"] if "material" in self.data_map[itemId] else ""
            style = self.data_map[itemId]["style"] if "style" in self.data_map[itemId] else ""
            keywords = self.data_map[itemId]["keywords"] if "keywords" in self.data_map[itemId] else ""

            query_object = {
                "user_query": user_input["user_query"],
                "material": material,
                "style": style,
                "keywords": keywords
            }
            boolean_query, object_description = await self.process_query(query_object)

            # Get embedding for the description
            query_embedding = await self.get_embedding(object_description)

            # Get results using boolean query and similarity
            results = self.retrieve_with_boolean_and_similarity(
                boolean_query,
                query_embedding,
                k=k
            )
            results = [(str(result[0]), result[1]) for result in results]
            results = sorted(results, key=lambda item: item[1], reverse=True)

            final_results = []
            for item_id, _ in results:
                item_description = self.data_map[item_id]["description"]
                image_id = self.image_mapping[item_id] if item_id in self.image_mapping else None
                final_results.append({"item_id": item_id, "description": item_description, "image_id": image_id})

            return final_results

        except Exception as e:
            print(f"Error in retrieval: {e}")
            raise

    def build_and_save_index(self, data_file: str):
        """Build inverse index from the embedded data JSON file and save it"""
        print("Building inverse index...")
        with open(data_file, 'r') as f:
            data = json.load(f)

        # Track all available keys
        all_keys = set()
        embeddings_data = []
        item_ids_list = []

        # Check if data is a list or dictionary
        if isinstance(data, list):
            print(f"Data is a list with {len(data)} items")
            for item in data:
                if 'item_id' not in item:
                    print(f"Warning: Item missing 'item_id' key: {item}")
                    continue

                item_id = item['item_id']
                self.items[item_id] = item
                all_keys.update(item.keys())

                # Process each relevant field
                fields = [
                    'color', 'description', 'item_keywords', 'item_shape',
                    'material', 'style', 'fabric_type', 'finish_type',
                    'pattern', 'dimensions'
                ]

                for field in fields:
                    if field in item:
                        # Handle both string and list values
                        values = item[field]
                        if isinstance(values, str):
                            values = [values]

                        # Process each value
                        for value in values:
                            if value:
                                # Convert to lowercase and split into words
                                words = re.findall(r'\w+', value.lower())
                                for word in words:
                                    if word not in self.index:
                                        self.index[word] = set()
                                    self.index[word].add(item_id)

                # Check for embedding data
                if 'embedding' in item:
                    embeddings_data.append(item['embedding'])
                    item_ids_list.append(item_id)
        else:
            # Handle as dictionary (original code)
            for item_id, item_data in data.items():
                self.items[item_id] = item_data
                all_keys.update(item_data.keys())

                # Process each relevant field
                fields = [
                    'color', 'description', 'item_keywords', 'item_shape',
                    'material', 'style', 'fabric_type', 'finish_type',
                    'pattern', 'dimensions'
                ]

                for field in fields:
                    if field in item_data:
                        # Handle both string and list values
                        values = item_data[field]
                        if isinstance(values, str):
                            values = [values]

                        # Process each value
                        for value in values:
                            if value:
                                # Convert to lowercase and split into words
                                words = re.findall(r'\w+', value.lower())
                                for word in words:
                                    if word not in self.index:
                                        self.index[word] = set()
                                    self.index[word].add(item_id)

                # Check for embedding data
                if 'embedding' in item_data:
                    embeddings_data.append(item_data['embedding'])
                    item_ids_list.append(item_id)

        # Save the inverse index
        with open(self.index_file, 'w') as f:
            json.dump({
                'index': {k: list(v) for k, v in self.index.items()},
                'items': self.items
            }, f)
        print(f"Inverse index saved to {self.index_file}")

        # Print available keys
        print(f"Available keys in the data: {all_keys}")

        # Build FAISS index if embeddings are available
        if embeddings_data:
            print(f"Found {len(embeddings_data)} items with embeddings")
            embeddings_array = np.array(embeddings_data)
            self.build_faiss_index(embeddings_array, item_ids_list)
        else:
            print("No embeddings found in the data")

    def load_index(self):
        """Load the pre-built inverse index"""
        if not os.path.exists(self.index_file):
            raise FileNotFoundError(f"Inverse index file {self.index_file} not found")

        with open(self.index_file, 'r') as f:
            data = json.load(f)
            self.index = {k: set(v) for k, v in data['index'].items()}
            self.items = data['items']

    def _evaluate_expression(self, terms: List[str]) -> Set[str]:
        """Evaluate a boolean expression without parentheses"""
        if not terms:
            return set()

        # Initialize result with first term's items
        result = self.index.get(terms[0].lower(), set())

        i = 1
        while i < len(terms):
            operator = terms[i].upper()
            if i + 1 >= len(terms):
                break

            next_term = terms[i + 1].lower()
            next_items = self.index.get(next_term, set())

            if operator == 'AND':
                result = result.intersection(next_items)
            elif operator == 'OR':
                result = result.union(next_items)
            elif operator == 'NOT':
                result = result.difference(next_items)

            i += 2

        return result

    def _process_parentheses(self, query: str) -> str:
        """Process parentheses in the query and evaluate sub-expressions"""
        # Remove extra spaces and normalize operators
        query = ' '.join(query.split())
        query = query.replace('( ', '(').replace(' )', ')')

        # Find the innermost parentheses
        while '(' in query:
            # Find the innermost parentheses
            start = query.rfind('(')
            end = query.find(')', start)

            if start == -1 or end == -1:
                break

            # Extract the sub-expression
            sub_expr = query[start + 1:end]

            # Evaluate the sub-expression
            sub_result = self._evaluate_expression(sub_expr.split())

            # Replace the sub-expression with a temporary token
            temp_token = f"__TEMP_{len(sub_result)}__"
            query = query[:start] + temp_token + query[end + 1:]

            # Store the result
            self.index[temp_token] = sub_result

        return query

    def boolean_query(self, query: str) -> Set[str]:
        """
        Process a boolean query with parentheses and return matching item_ids
        Query format: (word1 AND word2) OR (word3 AND NOT word4)
        """
        try:
            # Convert query to lowercase and remove extra spaces
            query = ' '.join(query.lower().split())

            # Process parentheses
            query = self._process_parentheses(query)

            # Split query into terms
            terms = query.split()

            # Evaluate the final expression
            result = self._evaluate_expression(terms)

            # Clean up temporary tokens
            for key in list(self.index.keys()):
                if key.startswith('__TEMP_'):
                    del self.index[key]

            return result

        except Exception as e:
            print(f"Error processing boolean query: {e}")
            return set()

    def build_faiss_index(self, embeddings: np.ndarray, item_ids: List[str]):
        """Build FAISS index from embeddings"""
        dimension = embeddings.shape[1]
        self.faiss_index = faiss.IndexFlatL2(dimension)
        self.faiss_index.add(embeddings)
        self.item_ids = item_ids
        self.embeddings = embeddings

        # Save embeddings for later use
        np.save(self.embeddings_file, embeddings)
        np.save(self.item_id_file, self.item_ids)
        print(f"FAISS index built with {len(item_ids)} items and {dimension} dimensions")

    def load_faiss_index(self):
        """Load pre-built FAISS index"""
        if not os.path.exists(self.embeddings_file):
            raise FileNotFoundError(f"Embeddings file {self.embeddings_file} not found")

        self.embeddings = np.load(self.embeddings_file)
        dimension = self.embeddings.shape[1]
        self.faiss_index = faiss.IndexFlatL2(dimension)
        self.faiss_index.add(self.embeddings)
        self.item_ids = np.load(self.item_id_file)
        print(f"FAISS index loaded with {self.embeddings.shape[0]} items and {dimension} dimensions")

    def retrieve_similar(self, query_embedding: np.ndarray, k: int = 10) -> List[Tuple[str, float]]:
        """
        Retrieve k most similar items using FAISS
        Returns list of (item_id, distance) tuples
        """
        if self.faiss_index is None:
            raise ValueError("FAISS index not initialized")

        distances, indices = self.faiss_index.search(query_embedding.reshape(1, -1), k)
        return [(self.item_ids[idx], float(dist)) for idx, dist in zip(indices[0], distances[0])]

    def retrieve_with_boolean_and_similarity(self,
                                             boolean_query: str,
                                             query_embedding: np.ndarray,
                                             k: int = 10) -> List[Tuple[str, float]]:
        """
        First apply boolean query to filter items, then rank by similarity
        Returns list of (item_id, distance) tuples
        """
        # Get items matching boolean query
        boolean_matches = self.boolean_query(boolean_query)

        if not boolean_matches:
            return []

        # Create a temporary FAISS index with only matching items
        matching_indices = [i for i, item_id in enumerate(self.item_ids) if item_id in boolean_matches]
        if not matching_indices:
            return []

        temp_index = faiss.IndexFlatL2(query_embedding.shape[0])
        temp_embeddings = self.embeddings[matching_indices]
        temp_index.add(temp_embeddings)

        # Search in the filtered index
        distances, local_indices = temp_index.search(query_embedding.reshape(1, -1), min(k, len(matching_indices)))

        # Map back to original item IDs
        return [(self.item_ids[matching_indices[idx]], float(dist))
                for idx, dist in zip(local_indices[0], distances[0])]


def build_index(data_file: str, index_file: str = 'new_inverse_index.json'):
    """One-time function to build and save the inverse index"""
    retrieval = SimpleRetrieval(index_file=index_file)
    retrieval.build_and_save_index(data_file)
    return retrieval


def main():
    parser = argparse.ArgumentParser(description='Build inverse index from embedded data')
    parser.add_argument('--data_file', type=str, default='image_embedding_data.json',
                        help='Path to the embedded data JSON file')
    parser.add_argument('--index_file', type=str, default='new_inverse_index.json',
                        help='Path to save the inverse index JSON file')
    parser.add_argument('--embeddings_file', type=str, default='embeddings.npy',
                        help='Path to save the embeddings numpy file')
    parser.add_argument('--item_id_file', type=str, default='item_ids.npy',
                        help='Path to save the embeddings numpy file')

    args = parser.parse_args()

    # Build and save the index
    retrieval = SimpleRetrieval(index_file=args.index_file, embeddings_file=args.embeddings_file, item_id_file=args.item_id_file)
    # retrieval.build_and_save_index(args.data_file)
    retrieval.load_index()
    retrieval.load_faiss_index()
    query = {
        "user_query": "yellow chair",
        "name": "chair",
        "material": "Hardwood/ Metal Base/ Polyester Fabric/ Foam Padding",
        "style": "chair",
        "keywords": ""
    }
    results = asyncio.run(retrieval.retrieve_with_query_object(query))
    print(results)
    print("Index building completed successfully!")


if __name__ == "__main__":
    main()
